# Hedgefund Research Platform (HRP) Specification

## Section 1: Vision & Principles

**Project Codename:** Hedgefund Research Platform (HRP)

**Mission:** A personal, professional-grade quantitative research platform that enables systematic discovery, validation, and deployment of long-only equity strategies with institutional rigor—built lean by integrating best-in-class existing tools.

### Core Principles

1. **Research-First** — Every trade idea starts as a hypothesis with a documented thesis, testable predictions, and falsification criteria. No "I think this works" without evidence.

2. **Reproducibility** — Every experiment is versioned, logged, and re-runnable. Code, data snapshots, parameters, and results are immutably linked. You can recreate any past result.

3. **Statistical Rigor** — Walk-forward validation, out-of-sample testing, and multiple hypothesis correction are enforced by default. The system assumes you'll overfit and protects you from yourself.

4. **Audit Trail** — Every decision, model version, and signal has lineage. If a strategy is deployed, you can trace exactly why it qualified—and why alternative strategies were rejected. Comparative reasoning, not cherry-picking.

5. **Leverage Existing Tools** — Don't rebuild what exists. Use VectorBT for backtesting, DuckDB for data, MLflow for experiments, Streamlit for dashboards. Build custom only where no good solution exists (agent integration, audit/lineage, orchestration).

6. **Hybrid Strategy Intelligence** — Combine ML-driven discovery with systematic application of established strategies (momentum, mean reversion, factor models). Classic strategies serve as baselines; agents surface novel hypotheses.

7. **Agent-Native** — AI agents are first-class citizens. Interactive research via MCP + scheduled background agents for monitoring and discovery. Both use the same underlying APIs as the dashboard.

8. **Local-First** — Runs entirely on your Mac Mini (M4, 16GB). Data stays private. No cloud dependencies for core functionality.

9. **Risk & Capital Discipline** — All strategies evaluated with realistic IBKR transaction costs. Signal-scaled position sizing with configurable limits. Performance assessed on post-cost, capacity-aware metrics.

### Scope Constraints

| Dimension | In Scope | Out of Scope (for now) |
|-----------|----------|------------------------|
| **Asset class** | US equities | ETFs (except benchmarks), crypto, futures, options |
| **Direction** | Long-only | Short selling, pairs trading |
| **Timeframe** | Daily (end-of-day signals) | Intraday, tick-level |
| **Universe** | S&P 500 initially, expand to Russell 1000/3000 | International equities |
| **Exclusions** | Financials, REITs, penny stocks (<$5) | — |
| **Broker** | Interactive Brokers | Other brokers |
| **Deployment** | Paper trading initially | Live trading (future phase) |

---

## Section 2: Architecture Overview

### Design Philosophy

Three-layer architecture that separates concerns while remaining simple enough for one person to maintain. Each layer has clear responsibilities and communicates through well-defined interfaces.

```
┌─────────────────────────────────────────────────────────────────┐
│                    CONTROL LAYER                                │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐ │
│  │   Streamlit     │  │   MCP Servers   │  │   Scheduled     │ │
│  │   Dashboard     │  │   (Claude)      │  │   Agents        │ │
│  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘ │
│           │                    │                    │          │
│           └────────────────────┼────────────────────┘          │
│                                ▼                               │
│                    ┌─────────────────────┐                     │
│                    │   Platform API      │                     │
│                    │   (Python module)   │                     │
│                    └──────────┬──────────┘                     │
└───────────────────────────────┼─────────────────────────────────┘
                                │
┌───────────────────────────────┼─────────────────────────────────┐
│                    RESEARCH LAYER                               │
│                                ▼                               │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐ │
│  │   VectorBT      │  │   MLflow        │  │   Hypothesis    │ │
│  │   (Backtest)    │  │   (Experiments) │  │   Registry      │ │
│  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘ │
│           │                    │                    │          │
│           └────────────────────┼────────────────────┘          │
│                                ▼                               │
│                    ┌─────────────────────┐                     │
│                    │   Audit/Lineage     │                     │
│                    │   System            │                     │
│                    └──────────┬──────────┘                     │
└───────────────────────────────┼─────────────────────────────────┘
                                │
┌───────────────────────────────┼─────────────────────────────────┐
│                    DATA LAYER                                   │
│                                ▼                               │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐ │
│  │   DuckDB        │  │   Ingestion     │  │   Feature       │ │
│  │   (Storage)     │  │   (Pipelines)   │  │   Store         │ │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘ │
└─────────────────────────────────────────────────────────────────┘
```

### Layer Responsibilities

#### Data Layer
| Component | Responsibility | Implementation |
|-----------|----------------|----------------|
| **DuckDB** | Store price data, fundamentals, signals, results | Single `.duckdb` file, SQL interface |
| **Ingestion Pipelines** | Fetch, validate, and load data from sources | Python scripts, scheduled via cron |
| **Feature Store** | Pre-computed features (momentum, volatility, etc.) | DuckDB tables, versioned by date |

#### Research Layer
| Component | Responsibility | Implementation |
|-----------|----------------|----------------|
| **VectorBT** | Backtesting engine | Vectorized, fast, handles position sizing |
| **MLflow** | Experiment tracking, model registry | Local server, logs params/metrics/artifacts |
| **Hypothesis Registry** | Track research questions and their status | DuckDB table + markdown docs |
| **Audit/Lineage** | Link hypothesis → experiment → result → decision | Custom, the core differentiator |

#### Control Layer
| Component | Responsibility | Implementation |
|-----------|----------------|----------------|
| **Platform API** | Single interface for all operations | Python module, used by all consumers |
| **Streamlit Dashboard** | Visualize data, experiments, results | Web UI on localhost |
| **MCP Servers** | Expose APIs to Claude for interactive research | FastMCP, tool definitions |
| **Scheduled Agents** | Background tasks: data refresh, monitoring, discovery | Python + Claude API, cron-triggered |

### Communication Patterns

1. **All consumers use the Platform API** — Dashboard, MCP servers, and agents never touch DuckDB or VectorBT directly. This ensures consistency and auditability.

2. **Synchronous for interactive** — Dashboard and MCP calls are synchronous, return results immediately.

3. **Async for batch** — Data ingestion and scheduled agents run asynchronously, write results to DuckDB, send notifications on completion/failure.

4. **Everything is logged** — Every API call is logged with timestamp, caller, parameters, and outcome.

### Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| Single DuckDB file | Simple, portable, no database server to manage. DuckDB handles analytical queries well. |
| VectorBT over custom | Battle-tested, vectorized (fast), good pandas integration. Don't rebuild backtesting. |
| MLflow over custom | Industry standard, handles versioning and comparison. Overkill features can be ignored. |
| Platform API as Python module | No HTTP overhead for local use. MCP servers wrap it, dashboard imports it. |
| MCP for Claude integration | Native Claude Code support, tools are just Python functions. |

---

## Section 3: Data Layer

### Overview

The Data Layer handles all data acquisition, storage, and feature engineering. Built on DuckDB for simplicity and performance—no database server to manage, just a single file that handles analytical queries efficiently.

### Storage Schema

#### Core Tables

```sql
-- Universe: which symbols are tradeable and when
CREATE TABLE universe (
    symbol VARCHAR NOT NULL,
    date DATE NOT NULL,
    in_universe BOOLEAN DEFAULT TRUE,
    exclusion_reason VARCHAR,  -- 'financial', 'reit', 'penny_stock', 'delisted'
    PRIMARY KEY (symbol, date)
);

-- Daily OHLCV price data
CREATE TABLE prices (
    symbol VARCHAR NOT NULL,
    date DATE NOT NULL,
    open DECIMAL(12,4),
    high DECIMAL(12,4),
    low DECIMAL(12,4),
    close DECIMAL(12,4),
    adj_close DECIMAL(12,4),
    volume BIGINT,
    source VARCHAR,            -- 'polygon', 'yfinance', etc.
    ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (symbol, date)
);

-- Fundamental data (quarterly, point-in-time)
CREATE TABLE fundamentals (
    symbol VARCHAR NOT NULL,
    report_date DATE NOT NULL,      -- when the data became known
    period_end DATE NOT NULL,       -- fiscal period end
    metric VARCHAR NOT NULL,        -- 'revenue', 'eps', 'pe_ratio', etc.
    value DECIMAL(18,4),
    source VARCHAR,
    ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (symbol, report_date, metric)
);

-- Pre-computed features
CREATE TABLE features (
    symbol VARCHAR NOT NULL,
    date DATE NOT NULL,
    feature_name VARCHAR NOT NULL,  -- 'momentum_20d', 'volatility_60d', etc.
    value DECIMAL(18,6),
    version VARCHAR,                -- feature computation version
    computed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (symbol, date, feature_name, version)
);

-- Corporate actions for adjustment
CREATE TABLE corporate_actions (
    symbol VARCHAR NOT NULL,
    date DATE NOT NULL,
    action_type VARCHAR NOT NULL,   -- 'split', 'dividend', 'spinoff'
    factor DECIMAL(12,6),           -- split ratio or dividend amount
    source VARCHAR,
    PRIMARY KEY (symbol, date, action_type)
);
```

#### Metadata Tables

```sql
-- Data source registry
CREATE TABLE data_sources (
    source_id VARCHAR PRIMARY KEY,
    source_type VARCHAR,            -- 'price', 'fundamental', 'corporate_action'
    api_name VARCHAR,
    last_fetch TIMESTAMP,
    status VARCHAR                  -- 'active', 'failed', 'rate_limited'
);

-- Ingestion log
CREATE TABLE ingestion_log (
    log_id INTEGER PRIMARY KEY,
    source_id VARCHAR,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    records_fetched INTEGER,
    records_inserted INTEGER,
    status VARCHAR,                 -- 'success', 'partial', 'failed'
    error_message VARCHAR
);
```

### Ingestion Pipelines

#### Pipeline Structure

```
data/
├── ingestion/
│   ├── prices.py          # Fetch daily prices
│   ├── fundamentals.py    # Fetch fundamental data
│   ├── universe.py        # Update universe membership
│   ├── corporate_actions.py
│   └── scheduler.py       # Cron-like scheduling
```

#### Ingestion Principles

1. **Idempotent** — Running the same ingestion twice produces the same result. Use upserts, not inserts.

2. **Point-in-time aware** — Fundamental data uses `report_date` (when known), not `period_end` (fiscal period). This prevents look-ahead bias.

3. **Source tracking** — Every record knows where it came from and when it was ingested.

4. **Incremental** — Only fetch new data, not full history on every run.

5. **Validation on ingest** — Reject obviously bad data (negative prices, missing required fields, dates in future).

#### Daily Schedule

| Time | Job | Description |
|------|-----|-------------|
| 6:00 PM ET | `prices.py` | Fetch today's closing prices |
| 6:30 PM ET | `universe.py` | Update universe membership |
| 7:00 PM ET | `features.py` | Recompute features for today |
| Weekly (Sat) | `fundamentals.py` | Fetch latest fundamental data |

### Feature Store

Pre-computed features avoid redundant calculation during backtests and research.

#### Standard Features

| Feature | Computation | Lookback |
|---------|-------------|----------|
| `return_Nd` | (close[t] - close[t-N]) / close[t-N] | 1, 5, 20, 60, 252 days |
| `momentum_Nd` | return_Nd | 20, 60, 252 days |
| `volatility_Nd` | std(returns) * sqrt(252) | 20, 60 days |
| `volume_avg_Nd` | mean(volume) | 20 days |
| `volume_ratio` | volume / volume_avg_20d | — |
| `rsi_Nd` | Relative Strength Index | 14 days |
| `sma_Nd` | Simple Moving Average | 20, 50, 200 days |
| `price_to_sma_Nd` | close / sma_Nd | 20, 50, 200 days |

#### Feature Versioning

Features are versioned to ensure reproducibility:
- `version` column tracks computation logic version
- Old versions retained for historical experiment reproduction
- New version computed alongside old until validated

### Data Quality

#### Automated Checks

| Check | Action |
|-------|--------|
| Missing prices for active symbol | Alert, mark data incomplete |
| Price change > 50% without corporate action | Flag for review |
| Negative prices or volume | Reject record |
| Duplicate records | Keep most recent ingestion |
| Gaps in date sequence | Alert, interpolation not automatic |

#### Quality Dashboard

Streamlit page showing:
- Ingestion status by source
- Data completeness by symbol
- Flagged anomalies requiring review
- Historical data quality trends

---

## Section 4: Research Engine

### Overview

The Research Engine is where hypotheses become evidence. Built on VectorBT for backtesting and MLflow for experiment tracking, with a custom Hypothesis Registry and Audit/Lineage system that ties everything together.

### Hypothesis Registry

Every research idea starts as a formal hypothesis before any code is written.

#### Hypothesis Schema

```sql
CREATE TABLE hypotheses (
    hypothesis_id VARCHAR PRIMARY KEY,  -- 'HYP-2025-001'
    title VARCHAR NOT NULL,
    thesis TEXT NOT NULL,               -- What you believe and why
    testable_prediction TEXT NOT NULL,  -- Specific, measurable outcome
    falsification_criteria TEXT,        -- What would prove this wrong
    status VARCHAR DEFAULT 'draft',     -- draft, testing, validated, rejected, deployed
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by VARCHAR,                 -- 'user', 'agent:discovery', etc.
    updated_at TIMESTAMP,
    outcome TEXT,                       -- Final conclusion
    confidence_score DECIMAL(3,2)       -- 0.00-1.00, post-validation
);

CREATE TABLE hypothesis_experiments (
    hypothesis_id VARCHAR,
    experiment_id VARCHAR,              -- MLflow run_id
    relationship VARCHAR,               -- 'primary', 'sensitivity', 'robustness'
    PRIMARY KEY (hypothesis_id, experiment_id)
);
```

#### Hypothesis Lifecycle

```
DRAFT → TESTING → VALIDATED/REJECTED → (if validated) → DEPLOYED
  │        │              │                                  │
  │        │              └── requires statistical evidence  │
  │        └── linked to experiments                         │
  └── requires thesis + prediction + falsification           │
                                                             └── user approval required
```

#### Example Hypothesis

```yaml
hypothesis_id: HYP-2025-003
title: "12-month momentum predicts 1-month forward returns"
thesis: |
  Stocks with strong 12-month returns (excluding last month) tend to
  continue outperforming over the next month. This is a well-documented
  anomaly attributed to investor underreaction.
testable_prediction: |
  A portfolio long top-decile momentum, rebalanced monthly, will
  outperform SPY by >3% annually (risk-adjusted) over 2010-2023.
falsification_criteria: |
  - Sharpe ratio < SPY Sharpe ratio
  - Alpha not statistically significant (p > 0.05)
  - Performance concentrated in <3 years (not robust)
```

### Backtesting with VectorBT

#### Backtest Configuration

```python
@dataclass
class BacktestConfig:
    # Universe
    symbols: list[str]
    start_date: date
    end_date: date

    # Strategy
    signal_function: Callable      # Returns signal DataFrame
    entry_threshold: float = 0.0   # Signal > threshold → enter
    exit_threshold: float = 0.0    # Signal < threshold → exit

    # Position sizing
    sizing_method: str = 'signal_scaled'  # 'equal', 'volatility', 'signal_scaled'
    max_position_pct: float = 0.10
    max_positions: int = 20

    # Costs (IBKR realistic)
    commission_pct: float = 0.0005  # 5 bps
    slippage_pct: float = 0.001     # 10 bps

    # Validation
    train_end: date                 # Walk-forward split
    test_start: date
```

#### Standard Metrics

Every backtest computes:

| Metric | Description |
|--------|-------------|
| `total_return` | Cumulative return over period |
| `cagr` | Compound annual growth rate |
| `sharpe_ratio` | Risk-adjusted return (rf=0) |
| `sortino_ratio` | Downside risk-adjusted return |
| `max_drawdown` | Worst peak-to-trough decline |
| `calmar_ratio` | CAGR / max drawdown |
| `win_rate` | % of profitable trades |
| `profit_factor` | Gross profit / gross loss |
| `avg_trade_return` | Mean return per trade |
| `trades_per_year` | Turnover indicator |
| `alpha` | Excess return vs benchmark |
| `beta` | Market sensitivity |
| `information_ratio` | Alpha / tracking error |

#### Benchmark Comparison

Every backtest automatically includes:
- **SPY buy-and-hold** — Always computed
- **Style benchmark** — If applicable (e.g., IWM for small-cap strategy)

### MLflow Integration

#### Experiment Structure

```
MLflow Tracking Server (local)
├── Experiment: "momentum-strategies"
│   ├── Run: "HYP-2025-003-v1" (params, metrics, artifacts)
│   ├── Run: "HYP-2025-003-v2-sensitivity"
│   └── Run: "HYP-2025-003-v3-robustness"
├── Experiment: "mean-reversion"
│   └── ...
```

#### What Gets Logged

| Category | Items |
|----------|-------|
| **Parameters** | All BacktestConfig fields, feature versions, universe snapshot |
| **Metrics** | All standard metrics (train and test periods separately) |
| **Artifacts** | Equity curve (PNG), trade log (CSV), signal distribution (PNG) |
| **Tags** | hypothesis_id, run_type (primary/sensitivity/robustness), status |

#### Reproducibility Contract

Any logged run can be exactly reproduced by:
1. Checking out the code version (git SHA logged)
2. Using the logged parameters
3. Using the logged data snapshot (feature versions)

### Audit/Lineage System

The core differentiator — every decision is traceable.

#### Lineage Schema

```sql
CREATE TABLE lineage (
    lineage_id INTEGER PRIMARY KEY,
    event_type VARCHAR NOT NULL,    -- 'hypothesis_created', 'experiment_run',
                                    -- 'validation_passed', 'deployment_approved'
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    actor VARCHAR,                  -- 'user', 'agent:discovery', 'system:scheduler'
    hypothesis_id VARCHAR,
    experiment_id VARCHAR,
    details JSONB,                  -- Event-specific metadata
    parent_lineage_id INTEGER       -- For causal chains
);
```

#### Lineage Queries

"Why was this strategy deployed?"
```sql
SELECT * FROM lineage
WHERE hypothesis_id = 'HYP-2025-003'
ORDER BY timestamp;
```

Returns the full chain: hypothesis creation → experiments → validation → approval → deployment.

"What experiments did the agent run last week?"
```sql
SELECT * FROM lineage
WHERE actor LIKE 'agent:%'
  AND event_type = 'experiment_run'
  AND timestamp > NOW() - INTERVAL '7 days';
```

### Research Workflow

#### Execution Permissions

| Step | Agent | User | Notes |
|------|-------|------|-------|
| **Hypothesize** | ✅ | ✅ | Agents can propose, users can create directly |
| **Experiment** | ✅ | ✅ | Either can trigger backtests |
| **Analyze** | ✅ | ✅ | Agents surface insights, users review |
| **Conclude** | ✅ | ✅ | Either can update hypothesis status |
| **Deploy** | ❌ | ✅ | **User approval required** — no autonomous trading |

#### Workflow Steps

```
1. HYPOTHESIZE (agent or user)
   └── Create hypothesis
   └── Define thesis, prediction, falsification
   └── Agent-created hypotheses tagged with source reasoning

2. EXPERIMENT (agent or user)
   └── Run primary backtest
   └── Log to MLflow, link to hypothesis
   └── Agents can batch-run multiple hypotheses

3. ANALYZE (agent or user)
   └── Review metrics vs. falsification criteria
   └── Run sensitivity tests (vary parameters)
   └── Run robustness tests (different periods, universes)
   └── Agents generate summary reports for user review

4. CONCLUDE (agent or user)
   └── Update hypothesis status (validated/rejected)
   └── Document outcome and confidence
   └── If validated: flag for user review

5. DEPLOY (user only)
   └── User reviews validated hypothesis
   └── User explicitly approves for paper trading
   └── Paper trade for N days
   └── User compares live vs. backtest
   └── User promotes to live or rejects
```

#### Agent Guardrails

- Agents **cannot** approve deployment
- Agents **cannot** modify deployed strategies
- Agents **cannot** disable risk limits
- All agent actions logged with full reasoning
- User can pause/disable agent research at any time

---

## Section 5: Control Layer (Dashboard + Agents)

### Overview

The Control Layer provides all interfaces to the platform: a Streamlit dashboard for visual interaction, MCP servers for Claude integration, and scheduled agents for autonomous research. All share the same Platform API, ensuring consistency and auditability.

### Platform API

The single interface that all consumers use. No direct database or VectorBT access from UI or agents.

#### API Structure

```python
# hrp/api/platform.py

class PlatformAPI:
    """Single entry point for all platform operations."""

    # Data operations
    def get_prices(self, symbols: list[str], start: date, end: date) -> pd.DataFrame
    def get_features(self, symbols: list[str], features: list[str], date: date) -> pd.DataFrame
    def get_universe(self, date: date) -> list[str]

    # Hypothesis operations
    def create_hypothesis(self, title: str, thesis: str, prediction: str,
                          falsification: str, actor: str) -> str  # returns hypothesis_id
    def update_hypothesis(self, hypothesis_id: str, status: str, outcome: str = None)
    def list_hypotheses(self, status: str = None) -> list[dict]
    def get_hypothesis(self, hypothesis_id: str) -> dict

    # Experiment operations
    def run_backtest(self, config: BacktestConfig, hypothesis_id: str = None,
                     actor: str = 'user') -> str  # returns experiment_id
    def get_experiment(self, experiment_id: str) -> dict
    def compare_experiments(self, experiment_ids: list[str]) -> pd.DataFrame

    # Deployment operations (user-only enforced at this layer)
    def approve_deployment(self, hypothesis_id: str, actor: str) -> bool
    def get_deployed_strategies(self) -> list[dict]

    # Lineage operations
    def get_lineage(self, hypothesis_id: str = None, experiment_id: str = None) -> list[dict]
    def log_event(self, event_type: str, actor: str, details: dict)
```

#### Actor Validation

```python
def approve_deployment(self, hypothesis_id: str, actor: str) -> bool:
    if actor.startswith('agent:'):
        raise PermissionError("Deployment requires user approval")
    # ... proceed with deployment
```

### Streamlit Dashboard

#### Pages

| Page | Purpose |
|------|---------|
| **Home** | System status, recent activity, alerts |
| **Data Health** | Ingestion status, data quality, completeness |
| **Universe** | Current tradeable symbols, exclusions, history |
| **Hypotheses** | Browse, create, update research hypotheses |
| **Experiments** | MLflow integration, compare runs, view artifacts |
| **Backtest** | Run new backtests, configure parameters |
| **Deployment** | Review validated strategies, approve/reject |
| **Lineage** | Trace decisions, audit trail explorer |
| **Agents** | Agent status, recent actions, pause/resume |
| **Settings** | API keys, notification preferences, risk limits |

#### Key Interactions

```
Dashboard                    Platform API                 Backend
    │                            │                           │
    │──[Create Hypothesis]──────►│                           │
    │                            │──[Insert + Log Event]────►│
    │◄─[hypothesis_id]───────────│                           │
    │                            │                           │
    │──[Run Backtest]───────────►│                           │
    │                            │──[VectorBT + MLflow]─────►│
    │◄─[experiment_id]───────────│                           │
    │                            │                           │
    │──[Approve Deployment]─────►│                           │
    │                            │──[Validate user actor]───►│
    │                            │──[Update status + Log]───►│
    │◄─[Success]─────────────────│                           │
```

### MCP Servers (Claude Integration)

MCP (Model Context Protocol) servers expose Platform API operations as tools that Claude can call.

#### Server Structure

```python
# hrp/mcp/research_server.py
from fastmcp import FastMCP

mcp = FastMCP("HRP Research")

@mcp.tool()
def create_hypothesis(title: str, thesis: str, prediction: str,
                      falsification: str) -> str:
    """Create a new research hypothesis."""
    api = PlatformAPI()
    return api.create_hypothesis(
        title=title, thesis=thesis, prediction=prediction,
        falsification=falsification, actor='agent:claude-interactive'
    )

@mcp.tool()
def run_backtest(hypothesis_id: str, symbols: list[str],
                 start_date: str, end_date: str,
                 signal_type: str = 'momentum_20d') -> dict:
    """Run a backtest for a hypothesis."""
    api = PlatformAPI()
    config = BacktestConfig(...)
    experiment_id = api.run_backtest(config, hypothesis_id,
                                      actor='agent:claude-interactive')
    return api.get_experiment(experiment_id)

@mcp.tool()
def analyze_results(experiment_id: str) -> str:
    """Get analysis summary for an experiment."""
    api = PlatformAPI()
    exp = api.get_experiment(experiment_id)
    # Format metrics, compare to benchmark, assess vs falsification criteria
    return format_analysis(exp)

# Note: No deploy tool exposed to agents
```

#### Available MCP Tools

| Tool | Description | Agent Access |
|------|-------------|--------------|
| `list_hypotheses` | Browse existing research | ✅ |
| `create_hypothesis` | Propose new research | ✅ |
| `get_universe` | Current tradeable symbols | ✅ |
| `get_features` | Retrieve feature data | ✅ |
| `run_backtest` | Execute backtest | ✅ |
| `analyze_results` | Get experiment analysis | ✅ |
| `compare_strategies` | Compare multiple experiments | ✅ |
| `update_hypothesis` | Change status, add notes | ✅ |
| `approve_deployment` | Deploy to paper trading | ❌ (not exposed) |

#### Claude Code Integration

```json
// ~/.claude/claude_desktop_config.json (or similar)
{
  "mcpServers": {
    "hrp-research": {
      "command": "python",
      "args": ["-m", "hrp.mcp.research_server"],
      "env": {
        "HRP_DB_PATH": "/path/to/hrp.duckdb"
      }
    }
  }
}
```

### Scheduled Agents

Background agents run autonomously on schedules, performing research tasks without user interaction.

#### Agent Types

| Agent | Schedule | Purpose |
|-------|----------|---------|
| **Data Monitor** | Every 6 hours | Check data freshness, alert on issues |
| **Discovery Agent** | Daily (overnight) | Scan for new hypothesis ideas from literature, patterns |
| **Validation Agent** | Daily | Run robustness checks on validated hypotheses |
| **Report Agent** | Weekly (Sunday) | Generate weekly research summary email |

#### Agent Implementation

```python
# hrp/agents/discovery_agent.py

class DiscoveryAgent:
    def __init__(self):
        self.api = PlatformAPI()
        self.claude = Anthropic()

    def run(self):
        """Discover new research hypotheses."""
        # Get current market context
        universe = self.api.get_universe(date.today())
        recent_features = self.api.get_features(universe, ALL_FEATURES, date.today())

        # Ask Claude to identify patterns worth investigating
        prompt = f"""
        Review this market data and suggest 3 research hypotheses
        worth testing. For each, provide:
        - Title
        - Thesis (why this might work)
        - Testable prediction
        - Falsification criteria

        Current universe: {len(universe)} stocks
        Feature summary: {summarize(recent_features)}
        """

        response = self.claude.messages.create(
            model="claude-sonnet-4-20250514",
            messages=[{"role": "user", "content": prompt}]
        )

        # Parse and create hypotheses
        hypotheses = parse_hypotheses(response.content)
        for h in hypotheses:
            self.api.create_hypothesis(
                **h, actor='agent:discovery'
            )

        # Log completion
        self.api.log_event('agent_run_complete', 'agent:discovery',
                          {'hypotheses_created': len(hypotheses)})
```

#### Scheduler

```python
# hrp/agents/scheduler.py
# Uses APScheduler or simple cron

from apscheduler.schedulers.background import BackgroundScheduler

scheduler = BackgroundScheduler()

scheduler.add_job(DataMonitorAgent().run, 'interval', hours=6)
scheduler.add_job(DiscoveryAgent().run, 'cron', hour=2)  # 2 AM
scheduler.add_job(ValidationAgent().run, 'cron', hour=3)  # 3 AM
scheduler.add_job(ReportAgent().run, 'cron', day_of_week='sun', hour=8)

scheduler.start()
```

### Notifications

Simple email-based notifications for important events.

#### Notification Events

| Event | Priority | Notification |
|-------|----------|--------------|
| Data ingestion failed | High | Immediate email |
| Hypothesis validated | Medium | Daily digest |
| Agent error | High | Immediate email |
| Weekly report ready | Low | Weekly email |
| Deployed strategy underperforming | High | Immediate email |

#### Implementation

```python
# hrp/notifications/email.py
import resend  # or smtplib for Gmail

def send_alert(subject: str, body: str, priority: str = 'medium'):
    if priority == 'high':
        resend.Emails.send({
            "from": "hrp@yourdomain.com",
            "to": "you@email.com",
            "subject": f"[HRP Alert] {subject}",
            "text": body
        })
    else:
        # Queue for digest
        queue_for_digest(subject, body)
```

### Remote Access

Access the dashboard from anywhere using Tailscale.

#### Setup

1. Install Tailscale on Mac Mini
2. Install Tailscale on mobile/laptop
3. Access dashboard at `http://mac-mini:8501` from any Tailscale-connected device

No port forwarding, no public exposure, encrypted tunnel.

---

## Section 6: ML Experiment Framework

### Overview

ML in trading is a minefield of overfitting. This framework prioritizes rigorous validation over model complexity, using MLflow to track everything and enforcing discipline that prevents fooling yourself.

### Hardware Constraints

M4 Mac Mini with 16GB RAM limits what's practical:

| Approach | Feasibility | Notes |
|----------|-------------|-------|
| Gradient boosting (XGBoost, LightGBM) | ✅ Excellent | Fast, memory-efficient, strong baseline |
| Random forests | ✅ Good | Moderate memory, parallelizes well |
| Linear models (Ridge, Lasso, ElasticNet) | ✅ Excellent | Fast, interpretable |
| Small neural networks | ⚠️ Limited | MPS acceleration helps, but keep small |
| Deep learning (transformers, LSTMs) | ❌ Not recommended | Memory and training time constraints |
| Large hyperparameter sweeps | ⚠️ Limited | Run overnight, constrain search space |

**Recommendation:** Start with gradient boosting (LightGBM) and linear models. These are often competitive with deep learning for tabular financial data and train in seconds/minutes, not hours.

### Model Types

#### Supported Models

```python
# hrp/ml/models.py

SUPPORTED_MODELS = {
    # Linear
    'ridge': Ridge,
    'lasso': Lasso,
    'elastic_net': ElasticNet,

    # Tree-based
    'lightgbm': LGBMRegressor,
    'xgboost': XGBRegressor,
    'random_forest': RandomForestRegressor,

    # Simple neural net (optional)
    'mlp': MLPRegressor,  # sklearn, CPU-friendly
}
```

#### Model Configuration

```python
@dataclass
class MLConfig:
    model_type: str
    target: str                    # 'return_5d', 'return_20d', etc.
    features: list[str]            # Feature names from feature store

    # Training
    train_start: date
    train_end: date
    validation_start: date
    validation_end: date
    test_start: date
    test_end: date

    # Model-specific hyperparameters
    hyperparameters: dict

    # Regularization
    feature_selection: bool = True
    max_features: int = 20         # Prevent overfitting with too many features
```

### Feature Engineering

#### Feature Categories

| Category | Examples | Notes |
|----------|----------|-------|
| **Momentum** | return_20d, return_60d, return_252d | Core signals |
| **Volatility** | volatility_20d, volatility_60d | Risk scaling |
| **Volume** | volume_ratio, volume_trend | Liquidity/interest |
| **Technical** | rsi_14, price_to_sma_50, price_to_sma_200 | Price patterns |
| **Cross-sectional** | momentum_rank, volatility_percentile | Relative positioning |

#### Feature Selection

Automatic feature selection to prevent overfitting:

```python
def select_features(X_train, y_train, max_features=20):
    """Select top features using mutual information."""
    mi_scores = mutual_info_regression(X_train, y_train)
    top_indices = np.argsort(mi_scores)[-max_features:]
    return X_train.columns[top_indices].tolist()
```

### Training Pipeline

```
1. LOAD DATA
   └── Pull features and targets from feature store
   └── Align dates, handle missing values

2. SPLIT (temporal, no shuffle)
   └── Train: 2010-2018
   └── Validation: 2018-2020 (hyperparameter tuning)
   └── Test: 2020-2023 (final evaluation, touch once)

3. FEATURE SELECTION
   └── Select on train set only
   └── Apply same features to validation/test

4. TRAIN
   └── Fit model on train set
   └── Log parameters to MLflow

5. VALIDATE
   └── Tune hyperparameters on validation set
   └── Use Optuna or simple grid search
   └── Log all trials to MLflow

6. EVALUATE
   └── Final evaluation on test set
   └── Compare to baseline (buy-and-hold, simple momentum)
   └── Log metrics and artifacts to MLflow

7. BACKTEST
   └── Convert predictions to trading signals
   └── Run through VectorBT with realistic costs
   └── This is the real test — prediction accuracy ≠ profitability
```

### Walk-Forward Validation

For production-ready strategies, use expanding or rolling walk-forward:

```python
def walk_forward_train(config: MLConfig, window_type='expanding'):
    """
    Train and evaluate using walk-forward methodology.

    Expanding: Train on all data up to t, predict t+1
    Rolling: Train on fixed window ending at t, predict t+1
    """
    results = []

    for train_end, test_start, test_end in generate_folds(config):
        # Train
        model = train_model(config, train_end=train_end)

        # Predict
        predictions = model.predict(get_features(test_start, test_end))

        # Evaluate
        fold_metrics = evaluate(predictions, actuals)
        results.append(fold_metrics)

        # Log to MLflow
        mlflow.log_metrics(fold_metrics, step=fold_number)

    return aggregate_results(results)
```

### MLflow Integration

#### Experiment Naming

```
MLflow Experiments
├── ml-momentum-prediction        # ML models predicting momentum-based returns
├── ml-mean-reversion            # ML models for mean reversion signals
├── ml-feature-research          # Feature importance and selection experiments
└── ml-ensemble                  # Combining multiple models
```

#### What Gets Logged

| Category | Items |
|----------|-------|
| **Parameters** | model_type, hyperparameters, features used, train/val/test dates |
| **Metrics** | MSE, MAE, R², IC (information coefficient), hit_rate, backtest_sharpe |
| **Artifacts** | Feature importance plot, prediction vs actual scatter, equity curve |
| **Tags** | hypothesis_id, walk_forward_fold, model_version |

#### Model Registry

Validated models are registered for deployment:

```python
# Only register if passes validation criteria
if metrics['test_sharpe'] > 0.5 and metrics['test_ic'] > 0.03:
    mlflow.register_model(
        f"runs:/{run_id}/model",
        f"strategy-{hypothesis_id}"
    )
```

### Preventing Overfitting

#### Mandatory Checks

| Check | Threshold | Action if Failed |
|-------|-----------|------------------|
| Train vs Test Sharpe ratio | Test < 0.5 × Train | Flag as overfit |
| Feature count | > 30 features | Require justification |
| Hyperparameter trials | > 100 trials | Require justification |
| Test set touched | > 3 times per hypothesis | Lock test set |
| Walk-forward consistency | Std(fold_sharpe) > mean | Flag as unstable |

#### Test Set Discipline

```python
class TestSetGuard:
    """Prevent excessive test set peeking."""

    def __init__(self, hypothesis_id: str):
        self.hypothesis_id = hypothesis_id
        self.test_evaluations = self._load_count()

    def evaluate_on_test(self, model, test_data):
        if self.test_evaluations >= 3:
            raise ValueError(
                f"Test set accessed {self.test_evaluations} times for "
                f"{self.hypothesis_id}. Further access requires explicit override."
            )
        self.test_evaluations += 1
        self._save_count()
        return model.evaluate(test_data)
```

### Signal Generation

Converting ML predictions to trading signals:

```python
def predictions_to_signals(predictions: pd.DataFrame,
                           method: str = 'rank') -> pd.DataFrame:
    """
    Convert raw predictions to actionable signals.

    Methods:
    - 'rank': Cross-sectional rank, go long top decile
    - 'threshold': Go long if prediction > threshold
    - 'zscore': Normalize predictions, signal = zscore
    """
    if method == 'rank':
        # Top 10% get signal = 1, rest = 0
        ranks = predictions.rank(pct=True, axis=1)
        return (ranks > 0.9).astype(float)
    elif method == 'threshold':
        return (predictions > threshold).astype(float)
    elif method == 'zscore':
        return (predictions - predictions.mean()) / predictions.std()
```

---

## Section 7: Risk & Validation Framework

### Overview

Two distinct concerns: **Risk Management** (protecting capital during trading) and **Validation** (ensuring strategies are statistically sound before deployment). Both are enforced by the platform, not left to human discipline.

---

## Part A: Risk Management

### Position Limits

Configurable per strategy, with hard platform-wide floors.

```python
@dataclass
class RiskLimits:
    # Position sizing
    max_position_pct: float = 0.10      # Max 10% in single position
    max_positions: int = 20             # Max simultaneous positions
    min_position_pct: float = 0.02      # Min 2% (avoid tiny positions)

    # Portfolio-level
    max_gross_exposure: float = 1.0     # 100% (no leverage for now)
    max_sector_exposure: float = 0.30   # Max 30% in one sector

    # Drawdown
    strategy_stop_loss: float = 0.15    # Pause strategy at 15% drawdown
    portfolio_stop_loss: float = 0.20   # Pause all trading at 20% drawdown
```

#### Hard Floors (Platform-Enforced)

These cannot be overridden, even by user:

| Limit | Value | Rationale |
|-------|-------|-----------|
| Max single position | 25% | Prevent catastrophic single-stock risk |
| Max gross exposure | 150% | Limit leverage |
| Min positions for deployment | 5 | Ensure diversification |

### Drawdown Controls

```python
class DrawdownMonitor:
    """Monitor and enforce drawdown limits."""

    def __init__(self, limits: RiskLimits):
        self.limits = limits
        self.high_water_mark = None
        self.current_drawdown = 0.0

    def update(self, portfolio_value: float) -> str:
        if self.high_water_mark is None:
            self.high_water_mark = portfolio_value

        self.high_water_mark = max(self.high_water_mark, portfolio_value)
        self.current_drawdown = (self.high_water_mark - portfolio_value) / self.high_water_mark

        if self.current_drawdown >= self.limits.portfolio_stop_loss:
            return 'HALT_ALL'  # Stop all trading
        elif self.current_drawdown >= self.limits.strategy_stop_loss:
            return 'HALT_STRATEGY'  # Stop this strategy
        else:
            return 'OK'
```

### Transaction Cost Model

Realistic IBKR costs for backtesting and live trading.

```python
@dataclass
class CostModel:
    # Commission (IBKR tiered)
    commission_per_share: float = 0.005  # $0.005/share
    commission_min: float = 1.00         # $1 minimum
    commission_max_pct: float = 0.01     # 1% of trade value max

    # Spread (estimated from historical data)
    spread_bps: float = 5                # 5 bps half-spread for liquid large-cap

    # Slippage (market impact)
    slippage_bps: float = 5              # 5 bps for small orders in liquid names

    def total_cost_bps(self, trade_value: float, shares: int) -> float:
        commission = max(self.commission_min,
                        min(shares * self.commission_per_share,
                            trade_value * self.commission_max_pct))
        commission_bps = (commission / trade_value) * 10000
        return commission_bps + self.spread_bps + self.slippage_bps
```

### Risk Dashboard Metrics

Real-time monitoring in Streamlit:

| Metric | Description | Alert Threshold |
|--------|-------------|-----------------|
| Current drawdown | From high-water mark | > 10% |
| Daily VaR (95%) | Expected daily loss | > 2% |
| Gross exposure | Total position value / NAV | > 95% |
| Position concentration | Largest position % | > 15% |
| Sector concentration | Largest sector % | > 25% |
| Correlation to SPY | Rolling 20-day | > 0.95 (too passive) |

---

## Part B: Statistical Validation

### Validation Requirements

A hypothesis cannot be marked "validated" without passing these checks.

#### Minimum Criteria

| Criterion | Threshold | Rationale |
|-----------|-----------|-----------|
| Out-of-sample Sharpe | > 0.5 | Minimum risk-adjusted return |
| Out-of-sample period | ≥ 2 years | Sufficient data for significance |
| Number of trades | ≥ 100 | Statistical power |
| Profit factor | > 1.2 | Wins must exceed losses meaningfully |
| Max drawdown | < 25% | Survivable drawdown |
| Win rate | > 40% | Not purely dependent on few big wins |

#### Statistical Significance

```python
def test_significance(strategy_returns: pd.Series,
                      benchmark_returns: pd.Series,
                      alpha: float = 0.05) -> dict:
    """
    Test if strategy significantly outperforms benchmark.
    """
    excess_returns = strategy_returns - benchmark_returns

    # T-test for mean excess return > 0
    t_stat, p_value = ttest_1samp(excess_returns, 0)
    p_value_one_sided = p_value / 2 if t_stat > 0 else 1 - p_value / 2

    # Bootstrap confidence interval for Sharpe
    sharpe_ci = bootstrap_sharpe_ci(strategy_returns, confidence=0.95)

    return {
        'excess_return_annualized': excess_returns.mean() * 252,
        't_statistic': t_stat,
        'p_value': p_value_one_sided,
        'significant': p_value_one_sided < alpha,
        'sharpe_95_ci_lower': sharpe_ci[0],
        'sharpe_95_ci_upper': sharpe_ci[1],
    }
```

### Multiple Hypothesis Correction

When testing many strategies, some will appear profitable by chance. Apply corrections.

#### Bonferroni Correction (Conservative)

```python
def bonferroni_threshold(base_alpha: float, num_hypotheses: int) -> float:
    """Adjusted significance threshold."""
    return base_alpha / num_hypotheses

# Example: Testing 20 hypotheses at α=0.05
# Adjusted threshold: 0.05 / 20 = 0.0025
# Strategy must have p-value < 0.0025 to be significant
```

#### False Discovery Rate (Less Conservative)

```python
def benjamini_hochberg(p_values: list[float], alpha: float = 0.05) -> list[bool]:
    """
    Control false discovery rate at alpha level.
    Returns which hypotheses to reject (True = significant).
    """
    n = len(p_values)
    sorted_indices = np.argsort(p_values)
    sorted_pvals = np.array(p_values)[sorted_indices]

    # Find largest k where p(k) <= k/n * alpha
    thresholds = [(i + 1) / n * alpha for i in range(n)]
    rejected = sorted_pvals <= thresholds

    # Find cutoff
    if rejected.any():
        max_rejected = np.max(np.where(rejected)[0])
        rejected = np.zeros(n, dtype=bool)
        rejected[:max_rejected + 1] = True

    # Map back to original order
    result = np.zeros(n, dtype=bool)
    result[sorted_indices] = rejected
    return result.tolist()
```

### Robustness Checks

Required before validation:

| Check | Method | Pass Criteria |
|-------|--------|---------------|
| **Parameter sensitivity** | Vary key parameters ±20% | Sharpe stays > 0.3 |
| **Time period stability** | Test on 3+ subperiods | Profitable in > 2/3 |
| **Universe stability** | Test on different subsets | Performance consistent |
| **Regime robustness** | Test in bull/bear/sideways | Not purely bull-market strategy |
| **Out-of-sample decay** | Compare IS vs OOS Sharpe | OOS > 50% of IS |

```python
def robustness_report(hypothesis_id: str) -> dict:
    """Generate comprehensive robustness analysis."""
    experiments = get_experiments_for_hypothesis(hypothesis_id)

    return {
        'parameter_sensitivity': analyze_parameter_sensitivity(experiments),
        'time_stability': analyze_time_periods(experiments),
        'regime_analysis': analyze_market_regimes(experiments),
        'oos_decay': calculate_oos_decay(experiments),
        'overall_pass': all_checks_pass(...)
    }
```

### Validation Workflow

```
1. PRIMARY BACKTEST
   └── Run on train+validation period
   └── Must meet minimum criteria

2. STATISTICAL TESTS
   └── Significance test vs benchmark
   └── Apply multiple hypothesis correction if many strategies tested

3. ROBUSTNESS CHECKS
   └── Parameter sensitivity
   └── Time period stability
   └── Regime analysis

4. FINAL OUT-OF-SAMPLE
   └── Test set evaluation (guarded, max 3 touches)
   └── Compare to in-sample performance
   └── Document decay ratio

5. VALIDATION DECISION
   └── All checks pass → Status: VALIDATED
   └── Any check fails → Status: REJECTED (with reason)
   └── Edge cases → Status: NEEDS_REVIEW (user decision)
```

### Validation Report Template

Generated automatically for each validated hypothesis:

```markdown
# Validation Report: HYP-2025-003

## Summary
- **Status:** VALIDATED
- **Confidence Score:** 0.72
- **Validated Date:** 2025-01-15
- **Validated By:** system (auto)

## Performance Metrics (Out-of-Sample: 2021-2023)
| Metric | Value | Threshold | Pass |
|--------|-------|-----------|------|
| Sharpe Ratio | 0.83 | > 0.5 | ✅ |
| CAGR | 12.4% | — | — |
| Max Drawdown | 18.2% | < 25% | ✅ |
| Win Rate | 54% | > 40% | ✅ |
| Profit Factor | 1.45 | > 1.2 | ✅ |
| Trade Count | 847 | ≥ 100 | ✅ |

## Statistical Significance
- Excess return vs SPY: 4.2% annualized
- t-statistic: 2.34
- p-value: 0.0098
- **Significant at α=0.05:** ✅

## Robustness
| Check | Result |
|-------|--------|
| Parameter sensitivity | PASS |
| Time period stability | PASS (3/3 subperiods) |
| Regime analysis | PASS (profitable in 2/3 regimes) |
| OOS decay | 0.62 (acceptable) |

## Recommendation
Approved for paper trading. Monitor for 30 days before live deployment consideration.
```

---

## Section 8: Technology Stack

### Overview

Lean stack prioritizing mature, well-maintained tools. Avoid bleeding-edge dependencies. Everything runs locally on macOS.

### Core Stack

| Layer | Tool | Version | Purpose |
|-------|------|---------|---------|
| **Language** | Python | 3.11+ | Primary language |
| **Database** | DuckDB | 1.0+ | Analytical data storage |
| **Backtesting** | VectorBT | 0.26+ | Vectorized backtesting engine |
| **ML Tracking** | MLflow | 2.10+ | Experiment tracking, model registry |
| **Dashboard** | Streamlit | 1.30+ | Web UI |
| **Agent Protocol** | FastMCP | 0.1+ | Claude MCP server |
| **Scheduling** | APScheduler | 3.10+ | Background job scheduling |

### Python Dependencies

#### Core

```txt
# requirements.txt

# Data
duckdb>=1.0.0
pandas>=2.0.0
numpy>=1.24.0
pyarrow>=14.0.0

# Backtesting
vectorbt>=0.26.0

# ML
scikit-learn>=1.3.0
lightgbm>=4.0.0
xgboost>=2.0.0
optuna>=3.4.0           # Hyperparameter tuning

# Experiment tracking
mlflow>=2.10.0

# Dashboard
streamlit>=1.30.0
plotly>=5.18.0

# Agent integration
fastmcp>=0.1.0
anthropic>=0.18.0       # Claude API

# Scheduling & notifications
apscheduler>=3.10.0
resend>=0.7.0           # Email notifications

# Utilities
python-dotenv>=1.0.0
pydantic>=2.5.0
loguru>=0.7.0           # Logging
httpx>=0.26.0           # HTTP client for data APIs
```

#### Data Sources

```txt
# Data provider SDKs
yfinance>=0.2.30        # Free Yahoo Finance data
polygon-api-client>=1.12.0  # Polygon.io (if subscribed)
alpaca-py>=0.13.0       # Alpaca Markets
```

#### Development

```txt
# requirements-dev.txt
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.12.0
ruff>=0.1.0
mypy>=1.8.0
pre-commit>=3.6.0
```

### Version Pinning Strategy

```txt
# Use >= for flexibility during development
# Pin exact versions in production lockfile (requirements.lock)

# Generate lockfile:
pip freeze > requirements.lock
```

### System Dependencies

| Dependency | Installation | Purpose |
|------------|--------------|---------|
| Python 3.11+ | `brew install python@3.11` | Runtime |
| Git | Pre-installed on macOS | Version control |
| Tailscale | `brew install tailscale` | Remote access |

### Development Environment

#### Recommended Setup

```bash
# Create virtual environment
python3.11 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install
```

#### IDE Configuration

VSCode recommended with extensions:
- Python (Microsoft)
- Pylance
- Black Formatter
- Ruff

```json
// .vscode/settings.json
{
    "python.defaultInterpreterPath": ".venv/bin/python",
    "python.formatting.provider": "black",
    "editor.formatOnSave": true,
    "[python]": {
        "editor.defaultFormatter": "ms-python.black-formatter"
    }
}
```

### Services & Ports

Local services when running:

| Service | Port | Command |
|---------|------|---------|
| Streamlit Dashboard | 8501 | `streamlit run hrp/dashboard/app.py` |
| MLflow UI | 5000 | `mlflow ui --backend-store-uri sqlite:///mlflow.db` |
| MCP Server | stdio | Launched by Claude Code |

### Data Storage Locations

```
~/hrp-data/                    # Data directory (outside repo)
├── hrp.duckdb                 # Main database
├── mlflow/                    # MLflow artifacts
│   ├── mlruns/
│   └── mlflow.db
├── backups/                   # Database backups
└── cache/                     # API response cache
```

### Backup Strategy

```bash
# Daily backup script (cron)
#!/bin/bash
BACKUP_DIR=~/hrp-data/backups
DATE=$(date +%Y%m%d)

# Backup DuckDB
cp ~/hrp-data/hrp.duckdb "$BACKUP_DIR/hrp-$DATE.duckdb"

# Backup MLflow
tar -czf "$BACKUP_DIR/mlflow-$DATE.tar.gz" ~/hrp-data/mlflow/

# Keep last 30 days
find "$BACKUP_DIR" -type f -mtime +30 -delete
```

### Hardware Utilization

M4 Mac Mini optimization:

| Resource | Usage |
|----------|-------|
| **CPU** | VectorBT backtests (vectorized), LightGBM training |
| **Memory** | DuckDB queries, pandas DataFrames (monitor with `htop`) |
| **MPS (GPU)** | Optional PyTorch/MLP training acceleration |
| **SSD** | DuckDB storage, MLflow artifacts |

#### Memory Management Tips

```python
# Use DuckDB for large queries instead of loading to pandas
con = duckdb.connect('hrp.duckdb')
result = con.execute("""
    SELECT symbol, date, close
    FROM prices
    WHERE date > '2020-01-01'
""").df()  # Only materialize what you need

# VectorBT memory optimization
vbt.settings.array_wrapper['freq'] = 'D'  # Daily frequency
vbt.settings.caching['enabled'] = False   # Disable caching for large backtests
```

---

## Section 9: Data Sources

### Overview

Budget: ~$100/month. Priority: reliable price data first, fundamentals second. Start with free sources, upgrade as needed.

### Data Source Comparison

#### Price Data

| Source | Cost | History | Quality | API Limits | Recommendation |
|--------|------|---------|---------|------------|----------------|
| **Yahoo Finance** | Free | 20+ years | Good* | Unofficial, may break | Development/backup |
| **Polygon.io** | $29/mo (Basic) | 5+ years | Excellent | 5 calls/min (Basic) | ✅ Primary |
| **Alpaca** | Free | 5+ years | Good | Generous | Good alternative |
| **Tiingo** | $30/mo | 20+ years | Excellent | 500/hour | Good alternative |

*Yahoo Finance: Adjusted prices may have issues; unofficial API can break without notice.

#### Fundamental Data

| Source | Cost | Coverage | Point-in-Time | Recommendation |
|--------|------|----------|---------------|----------------|
| **Yahoo Finance** | Free | Basic ratios | ❌ No | Basic use only |
| **Polygon.io** | $79/mo (Stocks Starter) | Full financials | ✅ Yes | ✅ If budget allows |
| **Tiingo** | $30/mo | Good coverage | ⚠️ Partial | Alternative |
| **Alpha Vantage** | $50/mo | Fundamentals | ❌ No | Not recommended |
| **SimFin** | Free/$15mo | Good | ✅ Yes | Budget option |

#### Reference Data

| Data | Source | Notes |
|------|--------|-------|
| S&P 500 constituents | Wikipedia / GitHub lists | Update monthly |
| Sector classifications | Yahoo Finance / Polygon | GICS sectors |
| Market holidays | `exchange_calendars` package | NYSE calendar |

### Recommended Setup

#### Tier 1: Free Start (~$0/month)

For initial development and learning:

```python
# Primary: Yahoo Finance (free, unofficial)
import yfinance as yf

# Backup: Alpaca (free, official)
from alpaca.data import StockHistoricalDataClient
```

**Limitations:**
- Yahoo may break without notice
- No point-in-time fundamentals
- Survivorship bias in historical constituents

#### Tier 2: Serious Research (~$30-60/month)

```python
# Price data: Polygon.io Basic ($29/mo)
from polygon import RESTClient

# OR Tiingo ($30/mo) - better historical depth
import requests  # Tiingo has simple REST API
```

**Unlocks:**
- Reliable, official API
- Better data quality
- Proper corporate actions

#### Tier 3: Full Research (~$80-110/month)

```python
# Polygon.io Stocks Starter ($79/mo)
# Includes fundamentals with point-in-time

# Plus: SimFin for backup fundamentals ($15/mo or free tier)
```

**Unlocks:**
- Point-in-time fundamentals
- Proper backtesting of fundamental strategies

### API Configuration

#### Environment Variables

```bash
# .env file (never commit to git)
POLYGON_API_KEY=your_key_here
ALPACA_API_KEY=your_key_here
ALPACA_SECRET_KEY=your_secret_here
TIINGO_API_KEY=your_key_here
```

#### Polygon.io Setup

```python
# hrp/data/sources/polygon_source.py
from polygon import RESTClient
from datetime import date
import os

class PolygonSource:
    def __init__(self):
        self.client = RESTClient(os.getenv('POLYGON_API_KEY'))

    def get_daily_bars(self, symbol: str, start: date, end: date) -> pd.DataFrame:
        """Fetch daily OHLCV data."""
        bars = self.client.get_aggs(
            ticker=symbol,
            multiplier=1,
            timespan="day",
            from_=start.isoformat(),
            to=end.isoformat(),
            limit=50000
        )

        df = pd.DataFrame([{
            'date': pd.to_datetime(b.timestamp, unit='ms').date(),
            'open': b.open,
            'high': b.high,
            'low': b.low,
            'close': b.close,
            'volume': b.volume,
            'vwap': b.vwap,
        } for b in bars])

        return df

    def get_splits(self, symbol: str) -> pd.DataFrame:
        """Fetch stock splits for adjustment."""
        splits = self.client.list_splits(ticker=symbol)
        return pd.DataFrame([{
            'date': s.execution_date,
            'split_ratio': s.split_to / s.split_from
        } for s in splits])
```

#### Yahoo Finance Setup (Free Backup)

```python
# hrp/data/sources/yfinance_source.py
import yfinance as yf

class YFinanceSource:
    def get_daily_bars(self, symbol: str, start: date, end: date) -> pd.DataFrame:
        """Fetch daily OHLCV data from Yahoo Finance."""
        ticker = yf.Ticker(symbol)
        df = ticker.history(start=start, end=end, auto_adjust=False)

        return df.reset_index().rename(columns={
            'Date': 'date',
            'Open': 'open',
            'High': 'high',
            'Low': 'low',
            'Close': 'close',
            'Adj Close': 'adj_close',
            'Volume': 'volume'
        })
```

### Data Quality Considerations

#### Known Issues by Source

| Source | Issue | Mitigation |
|--------|-------|------------|
| Yahoo Finance | Adjusted prices recalculated retroactively | Store raw + adjustment factors separately |
| Yahoo Finance | Unofficial API, may break | Have Alpaca as backup |
| Polygon Basic | 5-year history limit | Supplement with Yahoo for older data |
| All sources | Missing data for delistings | Track delisting dates, handle gracefully |

#### Survivorship Bias

**Problem:** Current S&P 500 list excludes companies that were removed (often due to failure).

**Mitigation:**
```python
# Store historical index membership
CREATE TABLE index_membership (
    index_name VARCHAR,     -- 'SP500', 'SP400', etc.
    symbol VARCHAR,
    start_date DATE,        -- When added to index
    end_date DATE,          -- When removed (NULL if current)
    PRIMARY KEY (index_name, symbol, start_date)
);

# When backtesting, use point-in-time membership
def get_universe_at_date(index_name: str, as_of: date) -> list[str]:
    return query("""
        SELECT symbol FROM index_membership
        WHERE index_name = ?
          AND start_date <= ?
          AND (end_date IS NULL OR end_date > ?)
    """, index_name, as_of, as_of)
```

**Historical S&P 500 data sources:**
- Siblis Research (paid)
- Academic datasets
- Manual reconstruction from SEC filings (time-consuming)

For MVP: Accept this limitation, document it, focus on liquid large-caps that are unlikely to delist.

### Rate Limiting

```python
# hrp/data/rate_limiter.py
from functools import wraps
import time

class RateLimiter:
    def __init__(self, calls_per_minute: int):
        self.calls_per_minute = calls_per_minute
        self.min_interval = 60.0 / calls_per_minute
        self.last_call = 0

    def wait(self):
        elapsed = time.time() - self.last_call
        if elapsed < self.min_interval:
            time.sleep(self.min_interval - elapsed)
        self.last_call = time.time()

# Usage
polygon_limiter = RateLimiter(calls_per_minute=5)  # Basic tier

def fetch_with_rate_limit(symbol):
    polygon_limiter.wait()
    return polygon_source.get_daily_bars(symbol, ...)
```

### Initial Data Load

```bash
# scripts/initial_data_load.sh

# 1. Load S&P 500 universe
python -m hrp.data.ingestion.universe --index SP500

# 2. Backfill price history (will take time with rate limits)
python -m hrp.data.ingestion.prices --start 2010-01-01 --source polygon

# 3. Compute initial features
python -m hrp.data.ingestion.features --start 2010-01-01

# 4. Verify data quality
python -m hrp.data.quality.check --report
```

Estimated time for initial load (S&P 500, 15 years): 2-4 hours with rate limiting.

---

## Section 10: Directory Structure

### Repository Layout

```
hrp/
├── README.md
├── CLAUDE.md                   # Claude Code project context
├── pyproject.toml              # Project metadata, dependencies
├── requirements.txt            # Pinned dependencies
├── requirements-dev.txt        # Dev dependencies
├── .env.example                # Template for environment variables
├── .gitignore
├── .pre-commit-config.yaml
│
├── docs/
│   ├── plans/
│   │   └── 2025-01-19-hrp-spec.md   # This specification
│   ├── hypotheses/             # Hypothesis documentation
│   │   └── HYP-2025-001.md
│   └── decisions/              # Architecture Decision Records
│       └── ADR-001-duckdb.md
│
├── hrp/                        # Main package
│   ├── __init__.py
│   │
│   ├── api/                    # Platform API layer
│   │   ├── __init__.py
│   │   └── platform.py         # PlatformAPI class
│   │
│   ├── data/                   # Data layer
│   │   ├── __init__.py
│   │   ├── db.py               # DuckDB connection management
│   │   ├── schema.py           # Table definitions
│   │   ├── sources/            # Data source adapters
│   │   │   ├── __init__.py
│   │   │   ├── base.py         # Abstract base class
│   │   │   ├── polygon_source.py
│   │   │   ├── yfinance_source.py
│   │   │   └── alpaca_source.py
│   │   ├── ingestion/          # Data ingestion pipelines
│   │   │   ├── __init__.py
│   │   │   ├── prices.py
│   │   │   ├── fundamentals.py
│   │   │   ├── universe.py
│   │   │   └── features.py
│   │   ├── quality/            # Data quality checks
│   │   │   ├── __init__.py
│   │   │   └── checks.py
│   │   └── rate_limiter.py
│   │
│   ├── research/               # Research engine
│   │   ├── __init__.py
│   │   ├── hypothesis.py       # Hypothesis registry
│   │   ├── backtest.py         # VectorBT wrapper
│   │   ├── metrics.py          # Standard metrics calculation
│   │   ├── benchmark.py        # Benchmark comparison
│   │   └── lineage.py          # Audit trail
│   │
│   ├── ml/                     # ML experiment framework
│   │   ├── __init__.py
│   │   ├── models.py           # Model registry
│   │   ├── features.py         # Feature engineering
│   │   ├── training.py         # Training pipeline
│   │   ├── validation.py       # Walk-forward validation
│   │   └── signals.py          # Prediction to signal conversion
│   │
│   ├── risk/                   # Risk management
│   │   ├── __init__.py
│   │   ├── limits.py           # Position/risk limits
│   │   ├── drawdown.py         # Drawdown monitoring
│   │   ├── costs.py            # Transaction cost model
│   │   └── validation.py       # Statistical validation
│   │
│   ├── dashboard/              # Streamlit dashboard
│   │   ├── __init__.py
│   │   ├── app.py              # Main Streamlit app
│   │   └── pages/
│   │       ├── home.py
│   │       ├── data_health.py
│   │       ├── universe.py
│   │       ├── hypotheses.py
│   │       ├── experiments.py
│   │       ├── backtest.py
│   │       ├── deployment.py
│   │       ├── lineage.py
│   │       ├── agents.py
│   │       └── settings.py
│   │
│   ├── mcp/                    # MCP servers for Claude
│   │   ├── __init__.py
│   │   └── research_server.py  # Research tools for Claude
│   │
│   ├── agents/                 # Scheduled agents
│   │   ├── __init__.py
│   │   ├── scheduler.py        # APScheduler setup
│   │   ├── data_monitor.py
│   │   ├── discovery_agent.py
│   │   ├── validation_agent.py
│   │   └── report_agent.py
│   │
│   ├── notifications/          # Alerts and notifications
│   │   ├── __init__.py
│   │   └── email.py
│   │
│   └── utils/                  # Shared utilities
│       ├── __init__.py
│       ├── config.py           # Configuration management
│       └── logging.py          # Logging setup
│
├── scripts/                    # CLI scripts
│   ├── initial_data_load.sh
│   ├── daily_update.sh
│   ├── backup.sh
│   └── start_services.sh
│
├── strategies/                 # Strategy implementations
│   ├── __init__.py
│   ├── base.py                 # Abstract strategy class
│   ├── momentum/
│   │   ├── __init__.py
│   │   └── classic_momentum.py
│   ├── mean_reversion/
│   │   └── __init__.py
│   └── ml_based/
│       └── __init__.py
│
├── notebooks/                  # Jupyter notebooks (exploration)
│   └── .gitkeep
│
└── tests/
    ├── __init__.py
    ├── conftest.py             # Pytest fixtures
    ├── test_api/
    ├── test_data/
    ├── test_research/
    ├── test_ml/
    ├── test_risk/
    └── test_strategies/
```

### CLAUDE.md

Project context file for Claude Code. Located at repository root.

```markdown
# HRP - Hedgefund Research Platform

## Project Overview

Personal quantitative research platform for systematic trading strategy development.
Long-only US equities, daily timeframe, institutional rigor.

## Architecture

Three-layer architecture:
1. **Data Layer** - DuckDB storage, ingestion pipelines, feature store
2. **Research Layer** - VectorBT backtesting, MLflow experiments, hypothesis registry
3. **Control Layer** - Streamlit dashboard, MCP servers, scheduled agents

All external access goes through `hrp/api/platform.py`. Never access DuckDB directly.

## Key Principles

1. **Research-First** - Every strategy starts as a formal hypothesis
2. **Reproducibility** - All experiments versioned and re-runnable
3. **Statistical Rigor** - Walk-forward validation, significance testing enforced
4. **Audit Trail** - Full lineage from hypothesis to deployment

## Agent Permissions

| Action | Agent | User |
|--------|-------|------|
| Create/run hypotheses | ✅ | ✅ |
| Run backtests | ✅ | ✅ |
| Analyze results | ✅ | ✅ |
| **Deploy strategies** | ❌ | ✅ |

Agents cannot approve deployments or modify deployed strategies.

## Code Conventions

- Python 3.11+
- Type hints required
- Black formatting (100 char line length)
- All database access through `hrp/api/platform.py`
- Log all significant actions to lineage table

## Common Tasks

### Run a backtest
```python
from hrp.api.platform import PlatformAPI
api = PlatformAPI()
experiment_id = api.run_backtest(config, hypothesis_id='HYP-2025-001')
```

### Create a hypothesis
```python
api.create_hypothesis(
    title="Momentum predicts returns",
    thesis="Stocks with high 12-month returns continue outperforming",
    prediction="Top decile momentum > SPY by 3% annually",
    falsification="Sharpe < SPY or p-value > 0.05",
    actor='user'  # or 'agent:discovery'
)
```

### Query data
```python
prices = api.get_prices(['AAPL', 'MSFT'], start_date, end_date)
features = api.get_features(['AAPL'], ['momentum_20d', 'volatility_60d'], date)
```

## File Locations

- Database: `~/hrp-data/hrp.duckdb`
- MLflow: `~/hrp-data/mlflow/`
- Logs: `~/hrp-data/logs/`

## Testing

```bash
pytest tests/ -v
```

## Services

| Service | Command | Port |
|---------|---------|------|
| Dashboard | `streamlit run hrp/dashboard/app.py` | 8501 |
| MLflow UI | `mlflow ui` | 5000 |

## Current Scope

- Universe: S&P 500 (excluding financials, REITs, penny stocks)
- Direction: Long-only
- Timeframe: Daily
- Broker: Interactive Brokers
```

### Data Directory (Outside Repo)

```
~/hrp-data/                     # Gitignored, not in repo
├── hrp.duckdb                  # Main database
├── mlflow/
│   ├── mlruns/                 # MLflow experiment data
│   └── mlflow.db               # MLflow tracking database
├── backups/
│   ├── hrp-20250119.duckdb
│   └── mlflow-20250119.tar.gz
├── cache/                      # API response cache
│   └── polygon/
└── logs/
    ├── ingestion.log
    └── agents.log
```

### Key Files Explained

| File | Purpose |
|------|---------|
| `CLAUDE.md` | Project context for Claude Code |
| `hrp/api/platform.py` | Single entry point for all operations |
| `hrp/data/db.py` | DuckDB connection pool, query helpers |
| `hrp/research/hypothesis.py` | CRUD for hypothesis registry |
| `hrp/research/backtest.py` | VectorBT configuration and execution |
| `hrp/ml/training.py` | ML training pipeline with MLflow logging |
| `hrp/dashboard/app.py` | Streamlit multipage app entry point |
| `hrp/mcp/research_server.py` | MCP tools for Claude integration |
| `hrp/agents/scheduler.py` | Background job scheduling |

### Configuration Files

#### pyproject.toml

```toml
[project]
name = "hrp"
version = "0.1.0"
description = "Hedgefund Research Platform"
requires-python = ">=3.11"

[tool.black]
line-length = 100
target-version = ['py311']

[tool.ruff]
line-length = 100
select = ["E", "F", "I", "N", "W"]

[tool.mypy]
python_version = "3.11"
strict = true
```

#### .gitignore

```gitignore
# Python
__pycache__/
*.py[cod]
.venv/
*.egg-info/

# Data (stored outside repo)
*.duckdb
mlruns/

# Environment
.env

# IDE
.vscode/
.idea/

# OS
.DS_Store
```

#### .pre-commit-config.yaml

```yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.12.0
    hooks:
      - id: black

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.8
    hooks:
      - id: ruff
        args: [--fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]
```

### Module Dependencies

```
api/platform.py
    ├── data/db.py
    ├── research/hypothesis.py
    ├── research/backtest.py
    ├── research/lineage.py
    ├── ml/training.py
    └── risk/limits.py

dashboard/app.py
    └── api/platform.py

mcp/research_server.py
    └── api/platform.py

agents/*.py
    └── api/platform.py
```

All external consumers (dashboard, MCP, agents) go through `api/platform.py`. No direct database access.

---

## Section 11: Development Phases

### Overview

Phased approach prioritizing a working system over completeness. Each phase delivers usable functionality. Don't move to the next phase until the current one is stable.

---

### Phase 0: Foundation ✅ COMPLETE

**Goal:** Project skeleton, development environment, basic data pipeline.

#### Deliverables

- [x] Repository setup (git, pyproject.toml, pre-commit)
- [x] CLAUDE.md and README.md
- [x] Virtual environment with core dependencies
- [x] DuckDB schema creation (`hrp/data/schema.py`)
- [x] Basic data source adapter (Yahoo Finance) (`hrp/data/sources/yfinance_source.py`)
- [x] Price ingestion for 10 test symbols
- [x] Verify data in DuckDB

#### Success Criteria

```bash
# Can run this and see data
python -c "
import duckdb
con = duckdb.connect('~/hrp-data/hrp.duckdb')
print(con.execute('SELECT COUNT(*) FROM prices').fetchone())
"
```

#### Exit Criteria

- [x] 10 symbols with 5+ years of price data loaded
- [x] Schema matches spec
- [x] All tests pass

---

### Phase 1: Core Research Loop ✅ COMPLETE

**Goal:** Run a simple backtest end-to-end, log to MLflow.

#### Deliverables

- [x] Platform API skeleton (`hrp/api/platform.py` - 1200+ lines)
- [x] VectorBT backtest wrapper (`hrp/research/backtest.py`)
- [x] Standard metrics calculation (`hrp/research/metrics.py`)
- [x] MLflow integration (`hrp/research/mlflow_utils.py`)
- [x] Simple momentum strategy implementation
- [x] Benchmark comparison (SPY) (`hrp/research/benchmark.py`)

#### Success Criteria

```bash
# Can run a backtest and see results in MLflow
python -m hrp.research.backtest --strategy momentum --start 2015-01-01
mlflow ui  # View results at localhost:5000
```

#### Exit Criteria

- [x] Backtest runs without errors
- [x] Results logged to MLflow with params, metrics, artifacts
- [x] Equity curve artifact generated
- [x] Benchmark comparison included

---

### Phase 2: Hypothesis & Lineage ✅ COMPLETE

**Goal:** Formal hypothesis workflow with audit trail.

#### Deliverables

- [x] Hypothesis registry (`hrp/research/hypothesis.py`)
- [x] Lineage system (`hrp/research/lineage.py`)
- [x] Link hypotheses to experiments
- [x] Hypothesis CRUD via Platform API
- [x] Basic validation criteria checks

#### Success Criteria

```python
# Can create hypothesis and link to experiment
api = PlatformAPI()
hyp_id = api.create_hypothesis(title="...", thesis="...", ...)
exp_id = api.run_backtest(config, hypothesis_id=hyp_id)
lineage = api.get_lineage(hypothesis_id=hyp_id)
assert len(lineage) >= 2  # creation + experiment
```

#### Exit Criteria

- [x] Hypothesis lifecycle works (draft → testing → validated/rejected)
- [x] All experiments linked to hypotheses
- [x] Lineage queries return correct chains

---

### Phase 3: Dashboard MVP ✅ COMPLETE

**Goal:** Basic Streamlit dashboard for visibility.

#### Deliverables

- [x] Streamlit app skeleton (`hrp/dashboard/app.py`)
- [x] Home page (system status) (`hrp/dashboard/pages/home.py`)
- [x] Data Health page (ingestion status) (`hrp/dashboard/pages/data_health.py`, `ingestion_status.py`)
- [x] Hypotheses page (list, create, view) (`hrp/dashboard/pages/hypotheses.py`)
- [x] Experiments page (MLflow integration) (`hrp/dashboard/pages/experiments.py`)

#### Success Criteria

```bash
streamlit run hrp/dashboard/app.py
# Navigate to localhost:8501, see data, create hypothesis
```

#### Exit Criteria

- [x] Dashboard loads without errors
- [x] Can view hypotheses and experiments
- [x] Can trigger backtest from UI

---

### Phase 4: Full Data Pipeline ✅ COMPLETE

**Goal:** Production-ready data ingestion with quality checks.

#### Deliverables

- [x] S&P 500 universe management (`hrp/data/universe.py`)
- [x] Polygon.io integration (or chosen paid source) (`hrp/data/sources/polygon_source.py`)
- [x] Feature store implementation (`hrp/data/ingestion/features.py` - 8 features)
- [x] Data quality checks (`hrp/data/quality/checks.py` - 5 checks)
- [x] Scheduled ingestion (daily cron) (`hrp/agents/scheduler.py`, `hrp/agents/jobs.py`)
- [x] Universe filtering (exclude financials, REITs, penny stocks)

#### Bonus Implementations
- [x] YFinance fallback source (`hrp/data/sources/yfinance_source.py`)
- [x] Corporate actions support (splits, dividends)
- [x] Quality report storage and health trends (`hrp/data/quality/report.py`)
- [x] Email notifications on job failure
- [x] CLI interface (`python -m hrp.agents.cli`)
- [x] Retry logic with exponential backoff

#### Success Criteria

```bash
# Scheduled job runs and updates data
python -m hrp.agents.cli run-now --job prices
python -m hrp.agents.cli run-now --job features
python -m hrp.agents.cli list-jobs
```

#### Exit Criteria

- [x] Full S&P 500 universe loaded
- [x] Features computed and stored
- [x] Quality checks passing
- [x] Daily updates automated

---

### Phase 5: ML Framework ⏳ MVP COMPLETE

**Goal:** ML training pipeline with proper validation.

#### Deliverables (MVP)

- [x] ML model registry (`hrp/ml/models.py`) - Ridge, Lasso, ElasticNet, RandomForest, MLP + optional LightGBM/XGBoost
- [x] Training pipeline (`hrp/ml/training.py`) - Data loading, train/val/test splits, metrics
- [x] Walk-forward validation (`hrp/ml/validation.py`) - Expanding/rolling windows, stability score
- [x] Feature selection (`hrp/ml/training.py:select_features`) - Mutual information based
- [x] Signal generation from predictions (`hrp/ml/signals.py`) - rank, threshold, zscore methods
- [ ] Overfitting guards (test set discipline) - Future

#### Success Criteria

```python
# Can train model and generate signals
from hrp.ml import MLConfig, train_model, predictions_to_signals

config = MLConfig(
    model_type='ridge',
    target='returns_20d',
    features=['momentum_20d', 'volatility_20d'],
    train_start=date(2015, 1, 1),
    train_end=date(2018, 12, 31),
    validation_start=date(2019, 1, 1),
    validation_end=date(2019, 12, 31),
    test_start=date(2020, 1, 1),
    test_end=date(2023, 12, 31),
)
result = train_model(config, symbols=['AAPL', 'MSFT'])
signals = predictions_to_signals(predictions_df, method='rank', top_pct=0.1)
```

#### Exit Criteria (MVP)

- [x] Linear models (Ridge, Lasso) working
- [x] Tree-based models (RandomForest) working
- [x] Walk-forward validation implemented
- [ ] Test set guard enforced - Future
- [ ] ML experiments logged to MLflow - Future (framework ready)

---

### Phase 6: Agent Integration

**Goal:** Claude can run research via MCP.

#### Deliverables

- [ ] MCP server (`hrp/mcp/research_server.py`)
- [ ] Tools: list_hypotheses, create_hypothesis, run_backtest, analyze_results
- [ ] Claude Code configuration
- [ ] Actor tracking (agent vs user)
- [ ] Deployment permission enforcement

#### Success Criteria

```
# In Claude Code session with MCP connected
> List all hypotheses
> Create a new hypothesis about RSI mean reversion
> Run a backtest for it
> Analyze the results
```

#### Exit Criteria

- Claude can perform full research loop
- All actions logged with correct actor
- Deployment blocked for agents

---

### Phase 7: Scheduled Agents

**Goal:** Autonomous background research.

#### Deliverables

- [ ] Scheduler setup (`hrp/agents/scheduler.py`)
- [ ] Data Monitor agent
- [ ] Discovery agent (hypothesis generation)
- [ ] Validation agent (robustness checks)
- [ ] Report agent (weekly summary)
- [ ] Email notifications

#### Success Criteria

```bash
# Agents run overnight, find results in morning
python -m hrp.agents.scheduler --run-once
# Check email for report
```

#### Exit Criteria

- Agents run on schedule without errors
- Discovery agent creates valid hypotheses
- Email notifications working

---

### Phase 8: Risk & Validation

**Goal:** Full statistical validation framework.

#### Deliverables

- [ ] Position limits enforcement
- [ ] Drawdown monitoring
- [ ] Transaction cost model (IBKR)
- [ ] Statistical significance testing
- [ ] Multiple hypothesis correction
- [ ] Robustness checks (parameter sensitivity, regime analysis)
- [ ] Validation reports

#### Success Criteria

```python
# Validation enforced
api.update_hypothesis(hyp_id, status='validated')
# Raises error if criteria not met
```

#### Exit Criteria

- Cannot validate hypothesis without meeting criteria
- Robustness report generated automatically
- Risk limits enforced in backtests

---

### Phase 9: Paper Trading (Future)

**Goal:** Deploy validated strategies to paper trading.

#### Deliverables

- [ ] IBKR paper trading connection
- [ ] Position tracking
- [ ] Order execution
- [ ] Live vs backtest comparison
- [ ] Performance monitoring

**Note:** This phase is out of initial scope. Build only after Phase 8 is stable and you have validated strategies worth testing.

---

### Phase Summary

| Phase | Focus | Key Outcome |
|-------|-------|-------------|
| 0 | Foundation | Data loads into DuckDB |
| 1 | Core Research | Backtest runs, logs to MLflow |
| 2 | Hypothesis | Formal research workflow |
| 3 | Dashboard | Visual interface |
| 4 | Data Pipeline | Production data quality |
| 5 | ML Framework | ML training with validation |
| 6 | Agent Integration | Claude runs research |
| 7 | Scheduled Agents | Autonomous discovery |
| 8 | Risk & Validation | Statistical rigor enforced |
| 9 | Paper Trading | Live deployment (future) |

### Recommended Approach

1. **Complete Phase 0-3 first** — This gives you a working research loop with visibility
2. **Use the system** — Run real research, find pain points
3. **Then prioritize** — Maybe ML (Phase 5) matters more than scheduled agents (Phase 7) for you
4. **Iterate** — Each phase will reveal requirements you didn't anticipate

### Anti-Patterns to Avoid

| Anti-Pattern | Why It's Bad | Instead |
|--------------|--------------|---------|
| Building all phases before using | Never discover real needs | Ship Phase 3, then iterate |
| Perfecting data pipeline before any backtests | Paralysis | Use Yahoo Finance, upgrade later |
| Building ML before simple strategies work | Complexity too early | Momentum baseline first |
| Optimizing performance before it's slow | Premature optimization | Profile first, optimize second |
| Adding features before core works | Scope creep | Finish phases in order |
